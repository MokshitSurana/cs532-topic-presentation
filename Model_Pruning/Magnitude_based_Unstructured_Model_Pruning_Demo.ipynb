{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Pruning Demonstration on GPT‑2\n",
        "\n",
        "In this notebook, we will:\n",
        "\n",
        "1. Install required libraries  \n",
        "2. Load a pre‑trained GPT‑2 model  \n",
        "3. Measure its original size (parameter count and non‑zero weights)  \n",
        "4. Apply global magnitude‑based pruning to its Linear layers  \n",
        "5. Compare effective parameter counts before and after pruning  \n",
        "6. (Optional) Remove pruning reparameterization to make sparsity permanent  \n",
        "7. Compare model outputs on a simple prompt before vs. after pruning  \n",
        "\n",
        "Model pruning works by zeroing out (or removing) weights whose magnitudes are below some threshold, yielding a sparse model that can be more efficient at inference time.\n"
      ],
      "metadata": {
        "id": "yu1jrZ8TcJbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Dependencies\n",
        "\n",
        "We'll need the Hugging Face **transformers** library for GPT‑2 and **torch** for pruning utilities.\n"
      ],
      "metadata": {
        "id": "LMb6VcjacRQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u18PSH2ccSBp",
        "outputId": "8ca1342c-412b-4569-dde9-b479dd470af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Libraries\n",
        "\n",
        "Bring in PyTorch, pruning tools, and the Hugging Face `transformers` API.\n"
      ],
      "metadata": {
        "id": "try5Ad2QcWlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
      ],
      "metadata": {
        "id": "MEfXny7XcUOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load Pre‑trained Model and Tokenizer\n",
        "\n",
        "We'll use the small `gpt2` model for a quick demo.  \n",
        "We also switch it to evaluation mode.\n"
      ],
      "metadata": {
        "id": "xNiqItG1caZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2\"\n",
        "tokenizer  = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model      = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# Keep a copy for “before‑pruning” comparisons\n",
        "model_before = copy.deepcopy(model)\n"
      ],
      "metadata": {
        "id": "mMIe_R92cbyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Count Effective Parameters (Original)\n",
        "\n",
        "We define a helper that walks through each `Linear` layer and counts:\n",
        "- **total** number of weights  \n",
        "- **non‑zero** weights in the **effective** parameter (`module.weight.data`)  \n",
        "This correctly accounts for any masks applied by pruning.\n"
      ],
      "metadata": {
        "id": "3WUjfjz8ccIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_effective_weights(m):\n",
        "    total, nonzero = 0, 0\n",
        "    for module in m.modules():\n",
        "        if isinstance(module, torch.nn.Linear):\n",
        "            w = module.weight.data\n",
        "            total   += w.numel()\n",
        "            nonzero += (w != 0).sum().item()\n",
        "    return total, nonzero\n",
        "\n",
        "orig_total, orig_nonzero = count_effective_weights(model)\n",
        "print(f\"Before pruning: total={orig_total:,}, non_zero={orig_nonzero:,} \"\n",
        "      f\"({100 * (orig_nonzero/orig_total):.1f}% dense)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "200IDJ0Rccj7",
        "outputId": "2518eb73-145a-4b73-d694-dfa207eb242b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before pruning: total=38,597,376, non_zero=38,597,376 (100.0% dense)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Apply Global Unstructured Pruning\n",
        "\n",
        "We collect **all** `weight` parameters from `Linear` modules and prune 30% of the smallest‑magnitude weights **globally**.\n"
      ],
      "metadata": {
        "id": "SEPhIJOjccxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gather (module, 'weight') pairs for pruning\n",
        "to_prune = [\n",
        "    (module, 'weight')\n",
        "    for module in model.modules()\n",
        "    if isinstance(module, torch.nn.Linear)\n",
        "]\n",
        "\n",
        "# Apply global L1‑unstructured pruning: zero out 30% of weights by magnitude\n",
        "prune.global_unstructured(\n",
        "    to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.3,\n",
        ")\n"
      ],
      "metadata": {
        "id": "5b6PyjEicc_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Count Effective Parameters After Pruning\n",
        "\n",
        "Now that we’ve applied the masks, count the **effective** non‑zero weights again.\n"
      ],
      "metadata": {
        "id": "mUhurCD4cdMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "post_total, post_nonzero = count_effective_weights(model)\n",
        "print(f\"After pruning:  total={post_total:,}, non_zero={post_nonzero:,} \"\n",
        "      f\"({100 * (post_nonzero/post_total):.1f}% dense)\")\n",
        "print(f\"Zeroed weights: {(post_total - post_nonzero):,} \"\n",
        "      f\"({100 * (1 - post_nonzero/post_total):.1f}% pruned)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNtilUBjcZF6",
        "outputId": "2aa3551c-74a8-4315-ae8b-314763506655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After pruning:  total=38,597,376, non_zero=27,018,163 (70.0% dense)\n",
            "Zeroed weights: 11,579,213 (30.0% pruned)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. (Optional) Remove Pruning Reparameterization\n",
        "\n",
        "Pruning in PyTorch uses a `weight_orig` parameter and a `weight_mask` buffer internally.  \n",
        "To make the sparsity permanent (and drop the extra buffers), remove the reparameterization:\n"
      ],
      "metadata": {
        "id": "y-HgAZDWcuUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for module, _ in to_prune:\n",
        "    prune.remove(module, 'weight')\n"
      ],
      "metadata": {
        "id": "cUmaL6cQcuij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Compare Model Outputs Before vs. After Pruning\n",
        "\n",
        "Finally, we generate text from the **unpruned** copy and the **pruned** model on the same prompt to observe any differences.\n",
        "\n",
        "> **Note:** Because we mutated `model` in place, we kept `model_before` for a clean comparison.\n"
      ],
      "metadata": {
        "id": "_eTNzf3ucuxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In a distant future, AI and humans\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Original model\n",
        "    out_before = model_before.generate(**inputs, max_new_tokens=40)\n",
        "    # Pruned model\n",
        "    out_after  = model.generate(**inputs, max_new_tokens=40)\n",
        "\n",
        "print(\"=== Original GPT‑2 Output ===\")\n",
        "print(tokenizer.decode(out_before[0], skip_special_tokens=True))\n",
        "print(\"\\n=== Pruned GPT‑2 Output ===\")\n",
        "print(tokenizer.decode(out_after[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-bYCRT9cvAD",
        "outputId": "3b3bba2f-624b-4666-e026-4beae0c069d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Original GPT‑2 Output ===\n",
            "In a distant future, AI and humans will be able to communicate with each other, and the AI will be able to communicate with humans.\n",
            "\n",
            "The AI will be able to communicate with humans, and the AI will be able to communicate\n",
            "\n",
            "=== Pruned GPT‑2 Output ===\n",
            "In a distant future, AI and humans will be able to communicate with each other using the same language.\n",
            "\n",
            "\"We're going to have a lot more interaction between humans and AI,\" said Dr. Michael Siegel, director of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this demonstration, we applied global magnitude‐based pruning to GPT‑2, zeroing out 30 % of its smallest‐magnitude weights while preserving its overall structure. After pruning, the model retained coherent generation—albeit with subtle differences in phrasing—showing that significant sparsity can be introduced without catastrophic quality loss. This workflow highlights how unstructured pruning can reduce model size and pave the way for faster, more efficient inference. Future steps include experimenting with different sparsity levels, structured pruning approaches, and fine‑tuning to recover any performance gaps.  \n"
      ],
      "metadata": {
        "id": "ErDeXqgufWXz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1PggkESfXxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}